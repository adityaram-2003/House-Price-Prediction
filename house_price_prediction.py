# -*- coding: utf-8 -*-
"""House Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1g2XuY6naiiei_LRef-zLa7zDTXBtHuc7

Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""Data Pre-Processing"""

data = pd.read_csv('https://raw.githubusercontent.com/SouravG/Housing-price-prediction-using-Regularised-linear-regression/master/Housing%20Price%20data%20set.csv')
data=data.drop(['Unnamed: 0','driveway','recroom','fullbase','airco','prefarea', 'gashw'],axis=1)
mean = data.mean()[0]
stddev = data.std()[0]
data = (data - data.mean())/data.std()

data.head()

"""Feature Extraction"""

data=np.asarray(data)
Y=data[:,0:1]
X=data[:,1:]
one = np.ones((len(X),1))
X = np.concatenate((one,X),axis=1)
split_ratio = 0.9
split = int(split_ratio * X.shape[0])
X_test = X[split+1:,:]
X_train = X[:split+1, :]
Y_test = Y[split+1:,:]
Y_train = Y[:split+1, :]

"""Error Function"""

# helper Functions
def computeCost(X,y,theta,lam):
    tobesummed = np.power(((X.dot(theta.T))-y),2)+lam*np.sum(np.power(theta,2))
    return np.sum(tobesummed)/(2 * len(X))

def denormalise_price(price):
    global mean
    global stddev
    ret = price * stddev + mean
    return ret

def computeError(predicted, actual):
    error = 0
    for i in range(len(predicted)):
        error += abs(actual[i] - predicted[i]) / actual[i]
    error /= len(actual)
    return error[0]

def plotGraph(x,y,labelX='X',labelY='Y',title='X vs Y'):
  fig, ax = plt.subplots()
  ax.plot(x, y, 'r')
  ax.set_xlabel(labelX)
  ax.set_ylabel(labelY)
  ax.set_title(title)

"""Gradient Descent - Regularisation"""

# Gradient Descent
def gradientDescent(X,y,theta,iters,alpha, lam):
    lam_matrix = lam * np.ones(theta.shape)
    lam_matrix[0][0] = 0
    for i in range(iters):
        theta = theta*(1- lam_matrix / len(X)) - (alpha/len(X)) * np.sum(X * (X @ theta.T - y), axis=0)

    return theta

"""Normal equation - Regularisation"""

#Normal equation
def normalEquation(X,Y,lam):
    lam_matrix = lam * np.identity(X.shape[1])
    lam_matrix[0][0] = 0
    theta = np.linalg.inv(X.T.dot(X) + lam_matrix).dot(X.T).dot(Y)
    return theta

"""Machine Learning Model:
LINEAR REGRESSION
Plotting Error vs Lambda Graph
"""

import numpy as np

theta = np.zeros((1, X.shape[1]))
alpha = 0.1  # Set the learning rate
iters = 500  # Define the number of iterations (epochs)
error_matrix = []
lam_range = 600  # Define the maximum lambda value

for lam in range(lam_range):
    g = gradientDescent(X_train, Y_train, theta, iters, alpha, lam)
    Cost = computeCost(X_train, Y_train, g, lam)

    Y_pred = X_test.dot(g.T)
    error = computeError(denormalise_price(Y_pred), denormalise_price(Y_test))
    error_matrix.append(error * 100)

optimal_lambda = 0
min_error = 9999

for i in range(len(error_matrix)):
    if error_matrix[i] < min_error:
        optimal_lambda = i
        min_error = error_matrix[i]

print("min Error  : ", min_error, '%')
print("Optimal Lambda : ", optimal_lambda)
plotGraph(np.arange(lam_range), error_matrix, 'lambda', 'error', 'lambda vs error')

"""Prediction Results

"""

print('pred price =',denormalise_price(Y_pred[2][0]),'actual price =',denormalise_price(Y_test[2][0]))

"""Plotting Error vs Lambda Graph using Normal Equation"""

error_mat = []
lam_range = 600

for lam in range(lam_range):
    theta = normalEquation(X, Y, lam)
    Cost = computeCost(X_train, Y_train, theta.T, lam)
    Y_pred = X_test.dot(theta)
    error = computeError(denormalise_price(Y_pred), denormalise_price(Y_test))
    error_mat.append(error * 100)

optimal_lambda = 0
min_error = 9999

for i in range(len(error_mat)):
    if error_mat[i] < min_error:
        optimal_lambda = i
        min_error = error_mat[i]

print("min Error : ", min_error)
print("Optimal Lambda : ", optimal_lambda)
plotGraph(np.arange(lam_range), error_mat, 'lambda', 'error', 'lambda vs error')

"""Final Prediction Results"""

print('pred price =',denormalise_price(Y_pred[2][0]),'actual price =',denormalise_price(Y_test[2])[0])